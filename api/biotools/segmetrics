{"name":"SegMetrics","description":"Image segmentation and object detection performance measures","homepage":"https://github.com/BMCV/segmetrics.py","biotoolsID":"segmetrics","biotoolsCURIE":"biotools:segmetrics","version":[],"otherID":[{"value":"DOI:10.5281/ZENODO.10111958","type":"doi","version":"1.4"}],"relation":[{"biotoolsID":"galaxy_image_analysis","type":"includedIn"}],"function":[{"operation":[{"uri":"http://edamontology.org/operation_3443","term":"Image analysis"}],"input":[],"output":[],"note":null,"cmd":"python -m 'segmetrics.cli' --help"}],"toolType":["Command-line tool","Library"],"topic":[],"operatingSystem":["Linux","Mac"],"language":["Python"],"license":"MIT","collectionID":[],"maturity":"Mature","cost":"Free of charge","accessibility":"Open access","elixirPlatform":[],"elixirNode":[],"elixirCommunity":[],"link":[{"url":"https://github.com/BMCV/segmetrics.py","type":["Repository"],"note":null},{"url":"https://github.com/BMCV/segmetrics.py/issues","type":["Issue tracker"],"note":null},{"url":"https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/imgteam/segmetrics/ip_segmetrics","type":["Galaxy service"],"note":null}],"download":[{"url":"https://anaconda.org/bioconda/segmetrics/files","type":"Software package","note":null,"version":null},{"url":"https://github.com/BMCV/segmetrics.py","type":"Source code","note":null,"version":null}],"documentation":[{"url":"https://segmetrics.readthedocs.io","type":["API documentation","Command-line options","Installation instructions","Quick start guide","User manual"],"note":null}],"publication":[{"doi":"10.1093/bioinformatics/btu080","pmid":null,"pmcid":null,"type":["Method"],"version":null,"note":"Describes the SEG performance measure.","metadata":{"title":"A benchmark for comparison of cell tracking algorithms","abstract":"Motivation: Automatic tracking of cells in multidimensional time-lapse fluorescence microscopy is an important task in many biomedical applications. A novel framework for objective evaluation of cell tracking algorithms has been established under the auspices of the IEEE International Symposium on Biomedical Imaging 2013 Cell Tracking Challenge. In this article, we present the logistics, datasets, methods and results of the challenge and lay down the principles for future uses of this benchmark. Results: The main contributions of the challenge include the creation of a comprehensive video dataset repository and the definition of objective measures for comparison and ranking of the algorithms. With this benchmark, six algorithms covering a variety of segmentation and tracking paradigms have been compared and ranked based on their performance on both synthetic and real datasets. Given the diversity of the datasets, we do not declare a single winner of the challenge. Instead, we present and discuss the results for each individual dataset separately. © 2014 The Author 2014.","date":"2014-06-01T00:00:00Z","citationCount":305,"authors":[{"name":"Maska M."},{"name":"Ulman V."},{"name":"Svoboda D."},{"name":"Matula P."},{"name":"Matula P."},{"name":"Ederra C."},{"name":"Urbiola A."},{"name":"Espana T."},{"name":"Venkatesan S."},{"name":"Balak D.M.W."},{"name":"Karas P."},{"name":"Bolckova T."},{"name":"Streitova M."},{"name":"Carthel C."},{"name":"Coraluppi S."},{"name":"Harder N."},{"name":"Rohr K."},{"name":"Magnusson K.E.G."},{"name":"Jalden J."},{"name":"Blau H.M."},{"name":"Dzyubachyk O."},{"name":"Krizek P."},{"name":"Hagen G.M."},{"name":"Pastor-Escuredo D."},{"name":"Jimenez-Carretero D."},{"name":"Ledesma-Carbayo M.J."},{"name":"Munoz-Barrutia A."},{"name":"Meijering E."},{"name":"Kozubek M."},{"name":"Ortiz-De-Solorzano C."}],"journal":"Bioinformatics"}},{"doi":"10.1109/isbi.2009.5193098","pmid":null,"pmcid":null,"type":["Method"],"version":null,"note":"Describes the detection-based performance measures.","metadata":{"title":"Nuclear segmentation in microscope cell images: A hand-segmented dataset and comparison of algorithms","abstract":"Image segmentation is an essential step in many image analysis pipelines and many algorithms have been proposed to solve this problem. However, they are often evaluated subjectively or based on a small number of examples. To fill this gap, we hand-segmented a set of 97 fluorescence microscopy images (a total of 4009 cells) and objectively evaluated some previously proposed segmentation algorithms. We focus on algorithms appropriate for high-throughput settings, where only minimal user intervention is feasible. The hand-labeled dataset (and all software used to compare methods) is publicly available to enable others to use it as a benchmark for newly proposed algorithms. © 2009 IEEE.","date":"2009-11-17T00:00:00Z","citationCount":151,"authors":[{"name":"Coelho L.P."},{"name":"Shariff A."},{"name":"Murphy R.F."}],"journal":"Proceedings - 2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro, ISBI 2009"}},{"doi":"10.1109/icip.2003.1246871","pmid":null,"pmcid":null,"type":["Method"],"version":null,"note":"Describes the Hausdorff distance as a performance measure.","metadata":null},{"doi":"10.1023/a:1007975324482","pmid":null,"pmcid":null,"type":["Method"],"version":null,"note":"Describes the quantile-based Hausdorff distance.","metadata":{"title":"Efficiently Locating Objects Using the Hausdorff Distance","abstract":"The Hausdorff distance is a measure defined between two point sets, here representing a model and an image. The Hausdorff distance is reliable even when the image contains multiple objects, noise, spurious features, and occlusions. In the past, it has been used to search images for instances of a model that has been translated, or translated and scaled, by finding transformations that bring a large number of model features close to image features, and vice versa. In this paper, we apply it to the task of locating an affine transformation of a model in an image; this corresponds to determining the pose of a planar object that has undergone weak-perspective projection. We develop a rasterised approach to the search and a number of techniques that allow us to locate quickly all transformations of the model that satisfy two quality criteria; we can also efficiently locate only the best transformation. We discuss an implementation of this approach, and present some examples of its use.","date":"1997-01-01T00:00:00Z","citationCount":195,"authors":[{"name":"Rucklidge W.J."}],"journal":"International Journal of Computer Vision"}}],"credit":[{"name":"Leonid Kostrykin","email":"leonid.kostrykin@bioquant.uni-heidelberg.de","url":"https://kostrykin.com","orcidid":"https://orcid.org/0000-0003-1323-3762","gridid":null,"rorid":null,"fundrefid":null,"typeEntity":"Person","typeRole":["Primary contact"],"note":null},{"name":"Thomas Wollmann","email":null,"url":null,"orcidid":"https://orcid.org/0000-0002-4741-3844","gridid":null,"rorid":null,"fundrefid":null,"typeEntity":null,"typeRole":["Contributor"],"note":null}],"community":null,"owner":"kostrykin","additionDate":"2023-11-14T16:53:06.354949Z","lastUpdate":"2023-11-14T20:33:24.898180Z","editPermission":{"type":"private","authors":[]},"validated":0,"homepage_status":0,"elixir_badge":0,"confidence_flag":null}